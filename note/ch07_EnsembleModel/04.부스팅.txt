04. 부스팅 기법 (Boosting) 

부스팅 기법의 일종인 에이다 부스트 (Ada Boost)의 작동과정: 


배깅과는 다르게 중복을 허용하지 않고 훈련 샘플로 부터 샘플을 추출한다. 

-> 개별 분류기 중 하나를 훈련. 이후 에러율을 기반으로 해당 분류기의 가중치를 계산 ( a = 0.5*log(1-e)/e ) 

-> 다시 중복을 허용하지 않고 샘플을 추출함. 여기에 이전 단계에서 잘못 분류한 데이터들의 50%를 추가 ( 추가한 오분류된 데이터들에 대해 큰 가중치를 부과한다) 

    두번째 분류기를 학습시킨다. 

-> 위 두 단계에서 잘못 분류한 데이터들로 세번째 분류기를 학습시킨다. 


** 부스팅 기법을 활용하는 알고리즘의 대표 적인것으로 

XGBoost , Gradient Boost, Ada Boost가 있음. 


최근 캐글에서 가장 인기 있는 부스팅 모델이 XGBoost 인데, XBoost 에 적용되는 하이퍼 파라미터 값 몇가지를 정리해 보겠다. 

eta : 학습률 (기본값 0.3)

gamma :  information gain에 더해주는 규제 파라미터. gamma 값이 커질 수록  더 이상 분기를 하지 않으려 하고, 따라서 보수적인 모델 형성. -> 과적합을 방지하는 차원에서 쓰임. 

lambda : L2 정규화 규제 파라미터 

alpha : L1 정규화 규제 파라미터 
 