03. 커널 PCA (Kernel PCA) 

PCA와 LDA 차원 축소 모형은 데이터가 선형적으로 구분 될 수 있다고 가정함. 

반면 선형적으로 구분이 불가능한 데이터에 대해서는 선형적으로 구분이 가능한 고차원으로 데이터들을 매핑하여 선형적으로 구분하고, 

다시 고차원에 매핑된 데이터들을 다시 원본 특성 공간으로 매핑함. 

-> 다만 이 원본 특성 공간의 데이터들을 고차원으로 매핑하고, 다시 저차원 원본 공간으로 매핑하는데에 드는 비용이 너무 큼. 

-> '커널트릭'을 이용한다


1. 커널 트릭

SVM 모델에서 결정경계의 식을 구하는 내용을 떠올려 보면, 결정경계의 식이 dual 식으로 나오고, 거기에 x_i, x_j 두 데이터 포인트의 내적값이 있는 것을 기억할 것이다. 

고차원 으로 이 데이터 포인트들을 매핑시키고 두 데이터 포인트들의 내적값( phi(x_i) %*% phi(x_j)  )을 구하기에 너무 많은 비용이 드므로, phi(x_i) %*% phi(x_j) 를 커널함수로

대체하여 데이터 포인트들을 굳이 고차원으로 일일이 매핑시키지 않고 원본 특성 차원에서 바로 연산을 수행한다. 


2. 자주 쓰이는 커널 함수들 

(1) 다항커널 (Polynomial Kernel) : 커널 함수 식 참조할 위키페이지 https://en.wikipedia.org/wiki/Polynomial_kernel

(2) 가우시안커널 (Radial Basis Function / RBF) : 커널 함수의 식은 https://en.wikipedia.org/wiki/Radial_basis_function_kernel 위키 페이지 참조 

사이킷런에서 쓰이는 위 커널 함수들에 대한 하이퍼 파라미터 변수들: 

C : lasso, ridge 회귀에 포함되는 규제 변수값 ( 주의 할 것은 원래 식에 포함된 람다 값의 역수인 C = 1/lambda 라는 점에 유의) 

kernel : 사용할 커널 함수 종류 ('linear', 'poly' , 'rbf', 'sigmoid') 

gamma: 다항함수 커널이나 방사기저 함수 또는 시그모이드 커널 함수에 포함된 감마 모수



** 커널 트릭을 적용하기 위해선 모든 데이터가 표준화 처리가 되어 집단의 평균이 0으로 맞춰져 있어야 한다!! 